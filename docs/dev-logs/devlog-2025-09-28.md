# Developer Log â€” 2025-09-28

## Tag: v0.1.0

Today we landed a major milestone for Amara Core:

- **LiteLLM proxy is alive and stable**:
  - Running OpenAI-only config (no Ollama for now).
  - Auth enforced via `LITELLM_MASTER_KEY`, proxied as `/api/llm/*`.
  - `/api/llm/health` returns 200 through nginx with Bearer token.
- **NGINX is SSE-safe**:
  - `gzip off`, `proxy_buffering off`, `X-Accel-Buffering no`.
  - Explicitly forwards `Authorization` header.
- **Guardrails all passing**:
  - Pre-commit validates Amara script metadata headers.
  - Forbidden `docker compose` (v1) references removed.
  - Secret scanning, Qdrant recreate_collection checks all clean.
- **Pipeline is green on master** ðŸŽ‰

## Onboarding Improvements

- Added `.env.example` with all required variables (OpenAI key, LiteLLM proxy key, storage paths, GitHub MCP, Qdrant).
- Updated `README.md` with a dedicated **Environment Setup** section for quick spin-up.
- Confirmed `.gitignore` exception to allow `.env.example` is working.

## Next Steps

- Extend smoke tests to hit `/api/llm/v1/chat/completions` (streaming).
- Tighten CORS to `amara.bodyhigh.com`.
- Add structured access logs & rotation under `app_data/*`.
- Revisit n8n container restart loop (permissions).
- Plan Ollama reintroduction once GPU/CPU runtime is available.

## Reflection

This tag represents the first healthy baseline of the Amara stack:

- NGINX reverse proxy + Cloudflare tunnel pattern is validated.
- LiteLLM is wired and tested with OpenAI.
- Guardrails are catching drift before it reaches master.
- Onboarding is now smoother with `.env.example` and updated README.

Itâ€™s a good point to branch for new features with confidence that infra and onboarding are stable.
