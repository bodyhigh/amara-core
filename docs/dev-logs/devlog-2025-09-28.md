# Developer Log â€” 2025-09-28

## Tag: v0.1.0

Today we landed a major milestone for Amara Core:

- **LiteLLM proxy is alive and stable**:
  - Running OpenAI-only config (no Ollama for now).
  - Auth enforced via `LITELLM_MASTER_KEY`, proxied as `/api/llm/*`.
  - `/api/llm/health` returns 200 through nginx with Bearer token.
- **NGINX is SSE-safe**:
  - `gzip off`, `proxy_buffering off`, `X-Accel-Buffering no`.
  - Explicitly forwards `Authorization` header.
- **Guardrails all passing**:
  - Pre-commit validates Amara script metadata headers.
  - Forbidden `docker[dash]]compose` (v1) references removed.
  - Secret scanning, Qdrant recreate_collection checks all clean.
- **Pipeline is green on master** ðŸŽ‰

## Next Steps

- Extend smoke tests to hit `/api/llm/v1/chat/completions` (streaming).
- Tighten CORS to `amara.bodyhigh.com`.
- Add structured access logs & rotation under `app_data/*`.
- Plan Ollama reintroduction once GPU/CPU runtime is available.

## Reflection

This tag represents the first healthy baseline of the Amara stack:

- NGINX reverse proxy + Cloudflare tunnel pattern is validated.
- LiteLLM is wired and tested with OpenAI.
- Guardrails are catching drift before it reaches master.

Itâ€™s a good point to branch for new features with confidence that infra is stable.
