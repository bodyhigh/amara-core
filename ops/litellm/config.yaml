# ops/litellm/config.yaml
model_list:
  - model_name: local/llama3.1-8b
    litellm_params:
      model: ollama/llama3.1:8b
      api_base: http://ollama:11434
      api_key: ollama

  - model_name: openai/main
    litellm_params:
      model: openai/gpt-4o   # adjust to the exact model you have enabled
      # api_key comes from env OPENAI_API_KEY

model_groups:
  - name: chat-default
    models: [local/llama3.1-8b, openai/main]
    fallback: true

router_settings:
  routing_strategy: latency_based
  timeout: 120
  max_retries: 2
  retry_codes: [408, 429, 500, 502, 503, 504]
  retry_interval: 1.0
