# --- Amara-Core Environment Example ---
# Copy this to `.env` and fill in values before running `make` targets.

# --- Storage root for large data (Qdrant, Ollama, n8n, artifacts) ---
AMARA_STORAGE=/mnt/storage

# --- Embedding / OpenAI ---
OPENAI_API_KEY=          # required if EMBED_MODE=openai
OPENAI_EMBED_MODEL=text-embedding-3-small

# --- Embedding Mode ---
# Options: openai | local | dry
# - openai: use OpenAI API (requires OPENAI_API_KEY)
# - local: use sentence-transformers (CPU embedding, offline)
# - dry:   write chunk manifest only (no vectors)
EMBED_MODE=openai

# Upsert vectors into Qdrant automatically (1 = enabled)
EMBED_QDRANT_UPSERT=1

# --- GitHub MCP adapter ---
GITHUB_OWNER=bodyhigh
GITHUB_REPO=amara-core
GITHUB_TOKEN=<your_github_pat_with_repo_scope>
MCP_GITHUB_PORT=8088

# --- Qdrant ---
# Preferred: full URL (http://host:port)
QDRANT_PORT=6333
QDRANT_URL=http://localhost:${QDRANT_PORT}
QDRANT_COLLECTION=amara_context_v1

# --- Local models (only if EMBED_MODE=local) ---
# LOCAL_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

# --- LiteLLM / Ollama (optional) ---
# Run, from the terminal: echo "LITELLM_PROXY_KEY=$(openssl rand -hex 24)" >> .env
LITELLM_PORT=4000
# this must match what NGINX forwards (since the Bearer token is proxied directly)
LITELLM_PROXY_KEY=
# OLLAMA_MODELS=llama3.1:8b

# # QDRANT_API_KEY=<optional_if_enabled>
# # # Set to your embedding size; if using OpenAI text-embedding-3-small it's 1536; large is 3072.
# # EMBEDDING_DIM=1536
# # # Keep CI read-only by default
# # EMBED_QDRANT_UPSERT=1
